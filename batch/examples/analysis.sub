# Condor Submission File
# Header for Jobs, defines global variables for use in this condor submission
#   anything defined here will be defined for the rest of this submission file
#   they can be re-defined and then the new value would be used for any later 'queue' commands

# These variables are general condor variables that are helpful for us
universe     = vanilla
requirements = Arch=="X86_64" && (Machine  !=  "zebra01.spa.umn.edu") && (Machine  !=  "zebra02.spa.umn.edu") && (Machine  !=  "zebra03.spa.umn.edu") && (Machine  !=  "zebra04.spa.umn.edu") && (Machine  !=  "caffeine.spa.umn.edu")
+CondorGroup = "cmsfarm"

# How much memory does this job require?
#   This is somewhat hard to determine since one does not normally track memory usage of one's programs.
#   2 Gb of RAM is a high upper limit, so this probably mean that less jobs will run in parallel, BUT
#   it helps make sure no jobs slow down due to low amount of available memory.
request_memory = 2 Gb

# This line keeps any jobs in a 'hold' state return a failure exit status
#   If you are developing a new executable (run script), this might need to be removed
#   until you get your exit statuses defined properly
on_exit_hold = (ExitCode != 0)

# This line tells condor whether we should be 'nice' or not.
#   Niceness is a way for condor to help determine how 'urgent' this job is
#   Please default to alwasy being nice
nice_user = True

# Define variables to hold some helpful full paths
hdfs_dir  = /hdfs/cms/user/$ENV(USER)/ldmx
local_dir = /local/cms/user/$ENV(USER)/ldmx
dir_of_run_fire = $(local_dir)/ldmx-scripts/batch

# Now our job specific information
#   'executable' is required by condor and that variable name cannot be changed
#   the other variable names are ours and can be changed and used in the rest of this file
executable    = $(dir_of_run_fire)/run_fire.sh
env_script    = $(local_dir)/stable-installs/v3.0.0-dev-eat/setup.sh
scratch_root  = /export/scratch/user/$ENV(USER)
output_dir    = $(hdfs_dir)/MY_ANALYSIS_HISTS/
config_script = $(output_dir)/detail/config.py

# This is the number of seconds to pause between starting jobs
#   It is helpful to have some lag time so that transferring large files can happen
#   without overloading the file system. This should be a small number and 
#   (if all the files you are reading from and writing to are on hdfs) 
#   can be set to zero or commented out.
next_job_start_delay = 1

# Maybe save the output, if pass a defined save_output variable on command line
if defined save_output
  output = $Ff(save_output)/$(Cluster)-$(Process).out
  error  = $Ff(save_output)/$(Cluster)-$(Process).out
endif

# Finally, we define the arguments to the executable we defined earlier
#   Notice that we can use the variables we have defined before or inside of the 'queue' command.
#   Condor will make the substitutions before starting the job
arguments = $(scratch_root)/$(Cluster)-$(Process) $(env_script) $(config_script) $(output_dir) $(input_files) 

# Determine how many files to submit per job
#   If the CLI parameter 'nfiles_per_job' is defined, use that
#   otherwise set it to 10
if defined nfiles_per_job
  nper = $(nfiles_per_job) 
else
  nper = 10
endif

# Submit the Jobs
#   Now we actually submit the jobs.
#
#   This is a bash script to group the listings of all files
#   in the input directories into lines with a maximum of nper.
#   If the number of files in the directory is not a mutliple of
#   nper, the last line will have less than nper in it.
#
#   The queue sub-command 'from' will run everything between the slice and
#   the vertical bar '|' through the terminal and pass each line as
#   the defined variable for the job.
#
#   The pythonic slicing [:$(max_jobs)] defines what rows from the bash
#   listing should be actually submitted.
#     If max_jobs is defined on the command line, we only submit the first max_jobs jobs.
#     If max_jobs is not defined, we submit all of the rows.
if defined max_jobs
  N = $(max_jobs)
else
  N = ""
endif
queue input_files from [:$(N)] bash $(dir_of_run_fire)/group_files.sh $(nper) PUT_INPUT_DIRECTORIES_HERE |
